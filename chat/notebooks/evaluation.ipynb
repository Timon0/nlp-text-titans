{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c89f605-39c3-489a-a58d-603cbf5fd3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from model import Model\n",
    "\n",
    "torch.set_printoptions(linewidth=1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e16d9a-293a-4766-aec0-bb920eb1860b",
   "metadata": {},
   "source": [
    "# Load trained model\n",
    "Load the model from a trained checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7011ff1-0dd9-4c14-88c5-e32d7410ba3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1acbb54e-fffb-4c3b-8cd5-2c6896516a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (model): PeftModelForSeq2SeqLM(\n",
       "    (base_model): LoraModel(\n",
       "      (model): T5ForConditionalGeneration(\n",
       "        (shared): Embedding(32128, 768)\n",
       "        (encoder): T5Stack(\n",
       "          (embed_tokens): Embedding(32128, 768)\n",
       "          (block): ModuleList(\n",
       "            (0): T5Block(\n",
       "              (layer): ModuleList(\n",
       "                (0): T5LayerSelfAttention(\n",
       "                  (SelfAttention): T5Attention(\n",
       "                    (q): Linear(\n",
       "                      in_features=768, out_features=768, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (v): Linear(\n",
       "                      in_features=768, out_features=768, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (relative_attention_bias): Embedding(32, 12)\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (1): T5LayerFF(\n",
       "                  (DenseReluDense): T5DenseGatedActDense(\n",
       "                    (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                    (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                    (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (act): NewGELUActivation()\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1-11): 11 x T5Block(\n",
       "              (layer): ModuleList(\n",
       "                (0): T5LayerSelfAttention(\n",
       "                  (SelfAttention): T5Attention(\n",
       "                    (q): Linear(\n",
       "                      in_features=768, out_features=768, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (v): Linear(\n",
       "                      in_features=768, out_features=768, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (1): T5LayerFF(\n",
       "                  (DenseReluDense): T5DenseGatedActDense(\n",
       "                    (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                    (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                    (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (act): NewGELUActivation()\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (final_layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (decoder): T5Stack(\n",
       "          (embed_tokens): Embedding(32128, 768)\n",
       "          (block): ModuleList(\n",
       "            (0): T5Block(\n",
       "              (layer): ModuleList(\n",
       "                (0): T5LayerSelfAttention(\n",
       "                  (SelfAttention): T5Attention(\n",
       "                    (q): Linear(\n",
       "                      in_features=768, out_features=768, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (v): Linear(\n",
       "                      in_features=768, out_features=768, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (relative_attention_bias): Embedding(32, 12)\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (1): T5LayerCrossAttention(\n",
       "                  (EncDecAttention): T5Attention(\n",
       "                    (q): Linear(\n",
       "                      in_features=768, out_features=768, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (v): Linear(\n",
       "                      in_features=768, out_features=768, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (2): T5LayerFF(\n",
       "                  (DenseReluDense): T5DenseGatedActDense(\n",
       "                    (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                    (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                    (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (act): NewGELUActivation()\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1-11): 11 x T5Block(\n",
       "              (layer): ModuleList(\n",
       "                (0): T5LayerSelfAttention(\n",
       "                  (SelfAttention): T5Attention(\n",
       "                    (q): Linear(\n",
       "                      in_features=768, out_features=768, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (v): Linear(\n",
       "                      in_features=768, out_features=768, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (1): T5LayerCrossAttention(\n",
       "                  (EncDecAttention): T5Attention(\n",
       "                    (q): Linear(\n",
       "                      in_features=768, out_features=768, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (v): Linear(\n",
       "                      in_features=768, out_features=768, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (2): T5LayerFF(\n",
       "                  (DenseReluDense): T5DenseGatedActDense(\n",
       "                    (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                    (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                    (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (act): NewGELUActivation()\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (final_layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model.load_from_checkpoint('flan-t5-base-batch4.ckpt')\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/flan-t5-base', truncation_side='left')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0d0c48-35df-4aab-9cbb-92d217086574",
   "metadata": {},
   "source": [
    "# Configure dialog with model\n",
    "Creating an action to easily \"communicate\" with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7f15943-c227-4fa0-afb6-f56821dfae6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = {}\n",
    "\n",
    "def dialog(topic, prompt):\n",
    "    with torch.no_grad():\n",
    "        current_context = ''\n",
    "\n",
    "        if topic in context:\n",
    "            context[topic].append(prompt)\n",
    "        else:\n",
    "            context[topic] = []\n",
    "            context[topic].append(prompt)\n",
    "\n",
    "        current_context = ' '.join(context[topic])\n",
    "        tokenized_input = tokenizer(current_context, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        generated = model.model.generate(**tokenized_input, do_sample=True, top_p=0.9, max_new_tokens=200, no_repeat_ngram_size =5)\n",
    "        print(generated.dtype)\n",
    "        generated_text = tokenizer.decode(generated[0], skip_special_token=True)\n",
    "        context[topic].append(generated_text)\n",
    "\n",
    "        return generated_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1079fc56-9cad-439c-807a-e1efab13e59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "<pad> The White House is a federal government that serves the American public. It is the centerpiece of the Obama administration's budget and strategy, and is backed by the White House's extensive range of policies and strategies, including tax cuts, tax reform, and reform of the economy. The White House provides a unique and expansive range of policies, policy ideas, and programs to help Americans achieve their goals, including tax cuts and reforms. It is also a venue for events and events that make Americans feel connected to the White House and its people. The White House offers a diverse range of policies, strategies, and strategies to help Americans achieve the goals they set out to achieve, such as reducing tax burdens, reducing bureaucracy, and providing an environment conducive to community and community engagement. It is also home to several groups, including the American Civil Liberties Union, National Human Rights League, the American Civil Libertarian Union, and the Office of\n"
     ]
    }
   ],
   "source": [
    "print(dialog('eval', 'What is the white house?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f63b18ca-83d0-43f2-88d6-5d329af4e52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "<pad> The White House is located on the third floor of the White House, with its entrance on a second floor facing the Capitol and its adjacent offices. The White House is also home to the Office of General Counsel, the Office of Management and Budget, the Office of International Trade, the White House Research Bureau, and the Office of International Relations. The White House also is home to the Office and Research Center, both of which are focused on the White House's mission and mission. The White House has a large and diverse collection of books, documents, and materials, including a comprehensive collection of books, journals, and publications on issues of international interest. The White House maintains a diverse collection of documents, such as the Civil War books, the Civil War textbook, and the White House Collection of World Records. The White House's Chancellor, William H. Rendell, hosts a weekly lunchtime program called \"Response to the Obama White House\n"
     ]
    }
   ],
   "source": [
    "print(dialog('eval', 'Who lives there?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74bbffc1-c66c-4df4-ad62-8f20d6c1da07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "<pad> Python is a Python programming language that can be used to build and write Python code. It provides an extensible and extensible framework for Python, and allows for the development and testing of Python code in an open-source environment. Python is often used to generate Python code that uses the Python programming language. It can be used to create dynamically generated Python code and provides a powerful graphical interface for writing and debugging Python code. Python also provides a lot of features for creating custom and dynamic Python code, such as: - Automatically generated Python code: Python provides the ability to create and implement new Python code, including features like: - Custom code: Pythonia supports the development of custom Python code, such that it can be written with a programming language like Python or Pascal, and includes the ability to build custom Python code in Python or Pascal. - Flexible code: Pythania provides a variety of\n"
     ]
    }
   ],
   "source": [
    "print(dialog('python', 'What is python for?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebd54644-4d00-4bd3-90e3-aa1f0852d2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "<pad> I'm an English English based English speaking professional who is currently working as a technical consultant for a British automotive company. I'm a current member of F1's steering committee, and I have been in the industry for several years. I'm currently employed by the F1 team as a consultant. I'm looking for experienced drivers to help me with my tasks. I'm also an experienced engineer and have been in the automotive industry for several years, primarily for automotive solutions. I'm interested in working with clients, providing information and advice, and providing guidance and assistance. I'm available to answer any questions you may have, and I can help you with any further tasks. I'll be available at any time to discuss any issues you may have. I'm always available for help or assistance. I'd be happy to offer help or assistance, and I can assist you in any way I can. I'm here\n"
     ]
    }
   ],
   "source": [
    "print(dialog('formel1', 'Who drives in F1?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e0580be-da98-4efe-af6c-0fe81f5a029c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "<pad> For every race in the formula 1, for every season of formula 1, for every race in formula 2, for every season of Formula 1, for every season in Formula 2, for every season in formula 4, for every season in a cycle, for every season in every formula 6, for every season in season 7, for every season in each cycle, for every year in the cycle, for every cycle in each cycle, in formula 7, for every season of every cycle, for every consecutive year in a cycle. For example, if you're in Formula 2, for example, there are seven seasons in formula 1, for formula 2: Formula 3: Formula 4: Formula 5: Formula 6: Formula 8: Formula 9: Formula 9: formula 8: Formula 9 8: Formula 9 - Formula 9 - F1 - F1 12: Formula 13 - Formula 16 - Formula 14 - Formula 15 - Formula 17 - Formula 16</s>\n"
     ]
    }
   ],
   "source": [
    "print(dialog('formel1', 'how many races are in one season of formula 1?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02597d5a-e394-4f27-9523-1fa560425d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "<pad> You can use any program and any operating system. First, you can use the “File” option to open and run a file. This will allow you to start writing the file, or run a command to run the file. To use a command, you can use “File” or “File”. Here are some steps to get started with Microsoft Word: 1. Start a new project: Start a new document in a new document and install it on your computer. 1. Open the document with the “File”. 2. Open the document in a folder and click the “File”, which will open the document in e.g. “File,” “File”, “File,” or “File”). 3. Open the document in an Excel document: Click the “File,” which will open the Excel document in a Excel document in e,g. “FILE,” or “FILE”). 4. Open the Excel\n"
     ]
    }
   ],
   "source": [
    "print(dialog('word', 'How can I use microsoft word?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8c3807b-fc79-4ad2-a7ef-4aa94e011c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "<pad> Yes, you can download and install the program. To begin, you can download the software from the Google Play Store. Download the application and install the software on your computer. This will open the program and launch it. It is important to note that you should consider purchasing the program and installed it on your computer before using it, since it requires additional permissions. It's important to note that it's important to make sure that the software you use is compatible with your operating system. Once you install the software, you'll be able to run it at any time using it. You may be able to use it from a web browser or from an app store, such as Microsoft Windows. You may also be able to purchase the software by signing in with your Google account. If you want to use it on a mobile device, you'll need to pay a subscription to the service. If you want, you can choose to use the program on the\n"
     ]
    }
   ],
   "source": [
    "print(dialog('word', 'Do I have to buy it to use it?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ce95655-8722-4f9d-8f69-cf58de3d819f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "<pad> if you want to download the program from the Google Play Store. To start, you can download the program from a website. You can purchase it from the website by using the \"Get My Office\" link. It is not necessary to purchase the software from the Google Play store. To begin, you can download and install the program from the Go-Store. This will download the application and install it on your computer. To begin, the application is installed on your computer. You can choose to download the program on the Google Play Store by clicking the \"Get My App\" link. You can choose from the following: - - - The Microsoft Office, available on Google Play Store, is available on the Microsoft Office website: https://www.microsoftoffice.com/ - The Microsoft Outlook, available on Google Exchange: https://www2.microsoftoffice.com - The Microsoft Word, available on Google Marketplace: https://www3.microsoftoffice.com There are\n"
     ]
    }
   ],
   "source": [
    "print(dialog('word', 'Where can I buy it?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d2d28e-faf5-4df7-b21b-61bbf7296fc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
