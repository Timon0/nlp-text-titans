{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c89f605-39c3-489a-a58d-603cbf5fd3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from model import Model\n",
    "\n",
    "torch.set_printoptions(linewidth=1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e16d9a-293a-4766-aec0-bb920eb1860b",
   "metadata": {},
   "source": [
    "# Load trained model\n",
    "Load the model from a trained checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7011ff1-0dd9-4c14-88c5-e32d7410ba3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1acbb54e-fffb-4c3b-8cd5-2c6896516a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (model): PeftModelForSeq2SeqLM(\n",
       "    (base_model): LoraModel(\n",
       "      (model): T5ForConditionalGeneration(\n",
       "        (shared): Embedding(32128, 768)\n",
       "        (encoder): T5Stack(\n",
       "          (embed_tokens): Embedding(32128, 768)\n",
       "          (block): ModuleList(\n",
       "            (0): T5Block(\n",
       "              (layer): ModuleList(\n",
       "                (0): T5LayerSelfAttention(\n",
       "                  (SelfAttention): T5Attention(\n",
       "                    (q): Linear(\n",
       "                      in_features=768, out_features=768, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (v): Linear(\n",
       "                      in_features=768, out_features=768, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (relative_attention_bias): Embedding(32, 12)\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (1): T5LayerFF(\n",
       "                  (DenseReluDense): T5DenseGatedActDense(\n",
       "                    (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                    (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                    (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (act): NewGELUActivation()\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1-11): 11 x T5Block(\n",
       "              (layer): ModuleList(\n",
       "                (0): T5LayerSelfAttention(\n",
       "                  (SelfAttention): T5Attention(\n",
       "                    (q): Linear(\n",
       "                      in_features=768, out_features=768, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (v): Linear(\n",
       "                      in_features=768, out_features=768, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (1): T5LayerFF(\n",
       "                  (DenseReluDense): T5DenseGatedActDense(\n",
       "                    (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                    (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                    (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (act): NewGELUActivation()\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (final_layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (decoder): T5Stack(\n",
       "          (embed_tokens): Embedding(32128, 768)\n",
       "          (block): ModuleList(\n",
       "            (0): T5Block(\n",
       "              (layer): ModuleList(\n",
       "                (0): T5LayerSelfAttention(\n",
       "                  (SelfAttention): T5Attention(\n",
       "                    (q): Linear(\n",
       "                      in_features=768, out_features=768, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (v): Linear(\n",
       "                      in_features=768, out_features=768, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (relative_attention_bias): Embedding(32, 12)\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (1): T5LayerCrossAttention(\n",
       "                  (EncDecAttention): T5Attention(\n",
       "                    (q): Linear(\n",
       "                      in_features=768, out_features=768, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (v): Linear(\n",
       "                      in_features=768, out_features=768, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (2): T5LayerFF(\n",
       "                  (DenseReluDense): T5DenseGatedActDense(\n",
       "                    (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                    (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                    (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (act): NewGELUActivation()\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1-11): 11 x T5Block(\n",
       "              (layer): ModuleList(\n",
       "                (0): T5LayerSelfAttention(\n",
       "                  (SelfAttention): T5Attention(\n",
       "                    (q): Linear(\n",
       "                      in_features=768, out_features=768, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (v): Linear(\n",
       "                      in_features=768, out_features=768, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (1): T5LayerCrossAttention(\n",
       "                  (EncDecAttention): T5Attention(\n",
       "                    (q): Linear(\n",
       "                      in_features=768, out_features=768, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (v): Linear(\n",
       "                      in_features=768, out_features=768, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (2): T5LayerFF(\n",
       "                  (DenseReluDense): T5DenseGatedActDense(\n",
       "                    (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                    (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                    (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (act): NewGELUActivation()\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (final_layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model.load_from_checkpoint('flan-t5-base-batch4.ckpt')\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/flan-t5-base', truncation_side='left')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0d0c48-35df-4aab-9cbb-92d217086574",
   "metadata": {},
   "source": [
    "# Configure dialog with model\n",
    "Creating an action to easily \"communicate\" with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7f15943-c227-4fa0-afb6-f56821dfae6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = {}\n",
    "\n",
    "def dialog(topic, prompt):\n",
    "    with torch.no_grad():\n",
    "        current_context = ''\n",
    "\n",
    "        if topic in context:\n",
    "            context[topic].append(prompt)\n",
    "        else:\n",
    "            context[topic] = []\n",
    "            context[topic].append(prompt)\n",
    "\n",
    "        current_context = ' '.join(context[topic])\n",
    "        tokenized_input = tokenizer(current_context, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        generated = model.model.generate(**tokenized_input, do_sample=True, top_p=0.9, max_new_tokens=200, no_repeat_ngram_size =5)\n",
    "        print(generated.dtype)\n",
    "        generated_text = tokenizer.decode(generated[0], skip_special_token=True)\n",
    "        context[topic].append(generated_text)\n",
    "\n",
    "        return generated_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1079fc56-9cad-439c-807a-e1efab13e59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "<pad> White House is the White House building in Washington, D.C., located on Washington’s North Side. It was built to house President Bush and is currently the White House’s first building. The White House is also known for its mission statement: “To build a more peaceful, prosperous and prosperous country, the white House is an invaluable tool for all Americans, including Americans in the United States and other countries.” The White House is the largest house in the United States. The White House was founded in 1867 to house President Bush’s administration, which includes the White House’ spokesman, Paul Ryan, and several members of his Cabinet. The White House also has a number of other offices, including the White House’ Senate, the White House’ White House’s Office of Budget and Budget, and the White House’ congressional office. The White House has several rooms, including the Whitehouse’s lobbyist, the White House Press Association and the White House\n"
     ]
    }
   ],
   "source": [
    "print(dialog('eval', 'What is the white house?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f63b18ca-83d0-43f2-88d6-5d329af4e52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "<pad> The White House is located in Washington, D.C., where President Bush was born. While many Americans currently live in Washington, D, they enjoy the privacy of the White House, and have access to the iconic building's unique spaces and facilities. One popular place to live in the White House is Washington Square Park, which is a popular spot for shopping and shopping. The White House has a lot of spaces for socializing, including an outdoor patio, and is located near the Washington Square Park Zoo. Other popular spots for socializing are the White House's new restaurants and hotels. One area where the White House is located, known as The Park, is the White House' Senate office. Additionally, there are several other spaces to live in, including a lobbyist's office, a conference room, and a small office. The White House is also home to the White House Press Association, which hosts a weekly news show on the White House.\n"
     ]
    }
   ],
   "source": [
    "print(dialog('eval', 'Who lives there?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74bbffc1-c66c-4df4-ad62-8f20d6c1da07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "<pad> Python is a code-driven platform used by the Python community to create and maintain code. Python is a distributed application that uses the Python syntax and compiled code to build and maintain code in Python. <unk>python <unk>http://python.com/<unk>pythonic <unk>pyton <unk>/python<unk> <unk>pytheon <unk>pypython_<unk>pythont_<unk>pythetons; <unk>pyhetons.com<unk>pythettons.com<unk>Pythetons;<unk>pythlets.com<unk>Pmythetons<unk>pythtons;<unk>psythetons;psythetes.com<unk>psythtons.com<unk>. <unk>pxythtons.pythetons.com<unk>t<unk>pythen.com<unk>pzthen.com;\n"
     ]
    }
   ],
   "source": [
    "print(dialog('python', 'What is python for?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebd54644-4d00-4bd3-90e3-aa1f0852d2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "<pad> Hallo mein Name ist flan-t5</s>\n"
     ]
    }
   ],
   "source": [
    "print(dialog('translation', 'Translate from german to english: Hallo mein Name ist flan-t5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e0580be-da98-4efe-af6c-0fe81f5a029c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "<pad> Hallo mein Name ist Flan-T5.</s>\n"
     ]
    }
   ],
   "source": [
    "print(dialog('translation', 'Translate it to french'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02597d5a-e394-4f27-9523-1fa560425d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "<pad> You can use a copy of the document to save the document, then to access the document by using the user's name and email address in the document. Here is a brief description of how to use Microsoft Word: Microsoft Word: <unk> <unk> Microsoft Word: Using Microsoft Word can be a great way to learn how to use the app and the app's functions, including: - <unk> Microsoft Word Search: <unk> Microsoft Word allows users to search for a specific app, such as a website or email, in a web browser, or through other online search engines. - <unk> <unk> <unk> Word Search: Using Microsoft Google Translate: Using Microsoft <unk> <unk> If you want to use the app or app, you can use the <unk> <unk> toolbar<unk> on your web browser or app, or download it from a website. - Scripting: Using Microsoft Chrome for editing and saving\n"
     ]
    }
   ],
   "source": [
    "print(dialog('word', 'How can I use microsoft word?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8c3807b-fc79-4ad2-a7ef-4aa94e011c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "<pad> Yes! You can download Microsoft Word from https://www.microsoft.com/ and install it on your computer. Once you're set up, you can take the.embc file to your computer and set it up on your computer. To use the app, you can download it from https://www1.microsoft.com/ to install it on your desktop computer. The installation process can take anywhere from two hours to three hours, depending on your computer. You can download Microsoft PowerPoint from https://www2.microsoft.com/ as a PDF file, or you can download it directly from https://www4.microsoft.com/ at your computer's website. Here is the complete installation process: If you don't see the installation, you can use Microsoft PowerPoint. You can download a copy of the Microsoft PowerPoint file on your computer, then use Microsoft PowerPoint. The file can be downloaded from https://www3.microsoft.com/ after you install the app on\n"
     ]
    }
   ],
   "source": [
    "print(dialog('word', 'Do I have to buy it to use it?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ce95655-8722-4f9d-8f69-cf58de3d819f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "<pad> You can download Microsoft Word from https://www.microsoft.com/ and install it on your computer. Once you're set up, you can take the.embc file to your computer and set it up on your computer. To use the app, you can download it from https://www1.microsoft.com/ to install it on your desktop computer. The installation process can take anywhere from two hours to three hours, depending on your computer. You can download Microsoft PowerPoint from https://www2.microsoft.com/ as a PDF file, or you can download it directly from https://www4.microsoft.com/ at your computer's website. Here is the complete installation process: If you don't see the installation, you can use Microsoft PowerPoint. You can download a copy of the Microsoft PowerPoint file on your computer, then use Microsoft PowerPoint. The file can be downloaded from https://www3.microsoft.com/ after you install it on your PC. You\n"
     ]
    }
   ],
   "source": [
    "print(dialog('word', 'Where can I buy it?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d2d28e-faf5-4df7-b21b-61bbf7296fc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
