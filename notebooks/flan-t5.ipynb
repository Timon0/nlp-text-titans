{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80edd978",
   "metadata": {},
   "source": [
    "# Flan-T5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6186a2d3",
   "metadata": {},
   "source": [
    "The T5 Tokenizer uses the SentencePiece Library, so install it first.\n",
    "To use INT8 weights, we have to install ´accelerate´ and ´bitsandbytes´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "487fbd37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\micha\\anaconda3\\lib\\site-packages (0.1.99)\n",
      "Requirement already satisfied: accelerate in c:\\users\\micha\\anaconda3\\lib\\site-packages (0.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\micha\\appdata\\roaming\\python\\python39\\site-packages (from accelerate) (1.22.2)\n",
      "Requirement already satisfied: torch>=1.4.0 in c:\\users\\micha\\anaconda3\\lib\\site-packages (from accelerate) (1.13.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\micha\\appdata\\roaming\\python\\python39\\site-packages (from accelerate) (21.3)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\micha\\anaconda3\\lib\\site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\micha\\anaconda3\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\micha\\appdata\\roaming\\python\\python39\\site-packages (from packaging>=20.0->accelerate) (3.0.7)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\micha\\anaconda3\\lib\\site-packages (from torch>=1.4.0->accelerate) (4.3.0)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.38.1-py3-none-any.whl (104.3 MB)\n",
      "Installing collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.38.1\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece\n",
    "#!pip install accelerate\n",
    "#!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fcc4de",
   "metadata": {},
   "source": [
    "## flan-t5-small FP32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a74c97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0033289",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_small = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "model_small = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39989827",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Answer the following question by reasoning step-by-step. The cafeteria had 23 apples. If they used 20 for lunch bought 6 more, how many apples do they have?\"\n",
    "input_ids = tokenizer_small(input_text, return_tensors=\"pt\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df34f451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flat-t5-small\n",
      "Input: Chasch du au schwiizerdütsch?\n",
      "Output: <pad> Chasch du au schwiizerdütsch?</s>\n"
     ]
    }
   ],
   "source": [
    "outputs = model_small.generate(input_ids, max_new_tokens=50)\n",
    "print('flat-t5-small')\n",
    "print(f'Input: {input_text}')\n",
    "print(f'Output: {tokenizer_small.decode(outputs[0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efac9158",
   "metadata": {},
   "source": [
    "## flan-t5-base FP32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53fbb193",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_base = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "model_base = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ef8e67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Chasch du au schwiizerdütsch?\"\n",
    "input_ids = tokenizer_base(input_text, return_tensors=\"pt\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f94479cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flat-t5-base\n",
      "Input: Chasch du au schwiizerdütsch?\n",
      "Output: <pad> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n"
     ]
    }
   ],
   "source": [
    "outputs = model_base.generate(input_ids, max_new_tokens=50)\n",
    "print('flat-t5-base')\n",
    "print(f'Input: {input_text}')\n",
    "print(f'Output: {tokenizer_base.decode(outputs[0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bef334",
   "metadata": {},
   "source": [
    "## flan-t5-base INT8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead188f8",
   "metadata": {},
   "source": [
    "_Does not work on CPU_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "749e03af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin C:\\Users\\micha\\anaconda3\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cpu.so\n",
      "CUDA SETUP: Loading binary C:\\Users\\micha\\anaconda3\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cpu.so...\n",
      "argument of type 'WindowsPath' is not iterable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\bitsandbytes\\cextension.py:33: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\n                        Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\n                        the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\n                        these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\n                        `device_map` to `from_pretrained`. Check\n                        https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n                        for more details.\n                        ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m tokenizer_base \u001b[38;5;241m=\u001b[39m T5Tokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/flan-t5-base\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m model_base \u001b[38;5;241m=\u001b[39m \u001b[43mT5ForConditionalGeneration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoogle/flan-t5-base\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_in_8bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\modeling_utils.py:2591\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2587\u001b[0m         device_map_without_lm_head \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   2588\u001b[0m             key: device_map[key] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m device_map\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m modules_to_not_convert\n\u001b[0;32m   2589\u001b[0m         }\n\u001b[0;32m   2590\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m-> 2591\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2592\u001b[0m                 \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2593\u001b[0m \u001b[38;5;124;03m                Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\u001b[39;00m\n\u001b[0;32m   2594\u001b[0m \u001b[38;5;124;03m                the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\u001b[39;00m\n\u001b[0;32m   2595\u001b[0m \u001b[38;5;124;03m                these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\u001b[39;00m\n\u001b[0;32m   2596\u001b[0m \u001b[38;5;124;03m                `device_map` to `from_pretrained`. Check\u001b[39;00m\n\u001b[0;32m   2597\u001b[0m \u001b[38;5;124;03m                https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\u001b[39;00m\n\u001b[0;32m   2598\u001b[0m \u001b[38;5;124;03m                for more details.\u001b[39;00m\n\u001b[0;32m   2599\u001b[0m \u001b[38;5;124;03m                \"\"\"\u001b[39;00m\n\u001b[0;32m   2600\u001b[0m             )\n\u001b[0;32m   2601\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m device_map_without_lm_head\n\u001b[0;32m   2603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_tf:\n",
      "\u001b[1;31mValueError\u001b[0m: \n                        Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\n                        the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\n                        these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\n                        `device_map` to `from_pretrained`. Check\n                        https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n                        for more details.\n                        "
     ]
    }
   ],
   "source": [
    "tokenizer_base = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "model_base = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\", device_map=\"auto\", load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24409de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"translate English to German: How old are you?\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac197e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model_base.generate(input_ids)\n",
    "print(tokenizer_base.decode(outputs[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
