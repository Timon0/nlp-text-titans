{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f0b7e91-2c35-437b-be31-6d49ce3912bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers -q\n",
    "!pip install accelerate -q\n",
    "!pip install bitsandbytes -q\n",
    "!pip install peft -q\n",
    "!pip install datasets -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4091fa-bc38-46d0-b8bc-d721f56e8541",
   "metadata": {},
   "source": [
    "## Inspiration\n",
    "\n",
    "https://github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/peft-flan-t5-int8-summarization.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c6fa365-b023-4fc4-96ab-deb418d71d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 116\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cextension.py:33: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/user/mjakober')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('8081/hub/api'), PosixPath('http'), PosixPath('//hub')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('gitlab.enterpriselab.ch'), PosixPath('4567/el-core/gpuhub/tf-pytorch-gpu-notebook')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('8081/hub/api/users/mjakober/activity'), PosixPath('http'), PosixPath('//hub')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/user/mjakober/oauth_callback')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//10.152.183.108'), PosixPath('8001'), PosixPath('tcp')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//10.152.183.91'), PosixPath('tcp'), PosixPath('80')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//10.152.183.37'), PosixPath('tcp'), PosixPath('8081')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//10.152.183.91'), PosixPath('443'), PosixPath('tcp')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//10.152.183.1'), PosixPath('443'), PosixPath('tcp')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "import torch\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training, TaskType\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import numpy as np\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55807147-6f4c-4f8f-be69-3e0d7f21fa5c",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86eaf238-ed51-40b8-9167-cfe9709dc476",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4361d2c-c1c6-4515-8a20-932870236537",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\", device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb3b8b3-4a90-42cb-93cb-43d57b4c75d6",
   "metadata": {},
   "source": [
    "## Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bb1edb0-e642-4609-9030-d0dec73992f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/jovyan/.cache/huggingface/datasets/OpenAssistant___parquet/OpenAssistant--oasst1-2960c57d7e52ab15/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    }
   ],
   "source": [
    "dataset_source = load_dataset(\"OpenAssistant/oasst1\", split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb1befa9-ea8c-409f-b258-da1e4eadeaf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jovyan/.cache/huggingface/datasets/OpenAssistant___parquet/OpenAssistant--oasst1-2960c57d7e52ab15/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-90069cfbf595d225.arrow\n",
      "Loading cached processed dataset at /home/jovyan/.cache/huggingface/datasets/OpenAssistant___parquet/OpenAssistant--oasst1-2960c57d7e52ab15/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-cd113ff85921e3e3.arrow\n"
     ]
    }
   ],
   "source": [
    "dataset_prompts = dataset_source.filter(lambda x: x[\"role\"] == \"prompter\")\n",
    "dataset_assistants = dataset_source.filter(lambda x: x[\"role\"] == \"assistant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08fd6254-af34-485d-8ced-812798815d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31525\n",
      "52912\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset_prompts))\n",
    "print(len(dataset_assistants))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "949bb73b-74d3-4f8d-8672-41e0d3dfd2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prompt = dataset_prompts.to_pandas()\n",
    "df_assistant = dataset_assistants.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b783d0fe-e130-49db-9b3b-522525b5914c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prompt['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ab3db96-a981-4864-9e43-0728a766608f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_prompt(value):\n",
    "    try:\n",
    "        return df_prompt[df_prompt['message_id'] == value]['text'].iloc[0]\n",
    "    except:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ec8e38f-472b-4dd1-91cb-7f948b488a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_assistant['prompt'] = df_assistant['parent_id'].map(find_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9022451-e4b0-4731-bd21-1f3498777315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['message_id', 'parent_id', 'user_id', 'created_date', 'text', 'role',\n",
       "       'lang', 'review_count', 'review_result', 'deleted', 'rank', 'synthetic',\n",
       "       'model_name', 'detoxify', 'message_tree_id', 'tree_state', 'emojis',\n",
       "       'labels', 'prompt'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_assistant.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e29674ef-14a6-4d14-96e4-71c76273f64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_assistant.drop(['message_id', 'parent_id', 'user_id', 'created_date', 'role',\n",
    "       'lang', 'review_count', 'review_result', 'deleted', 'rank', 'synthetic', 'model_name',\n",
    "       'detoxify', 'message_tree_id', 'tree_state', 'emojis', 'labels'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d480a53-47cc-4c66-8b1b-3e465b1cdac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_assistant = df_assistant.astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f92b1f79-386d-4ad5-a876-2fbb46c3b5ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text      string[python]\n",
       "prompt    string[python]\n",
       "dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_assistant.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3fbd9b2-cf0b-4abd-9a5f-34a385123b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df_assistant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57ad92cd-3201-4f93-a6f4-3a956fedbab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '\"Monopsony\" refers to a market structure where there is only one buyer for a particular good or service. In economics, this term is particularly relevant in the labor market, where a monopsony employer has significant power over the wages and working conditions of their employees. The presence of a monopsony can result in lower wages and reduced employment opportunities for workers, as the employer has little incentive to increase wages or provide better working conditions.\\n\\nRecent research has identified potential monopsonies in industries such as retail and fast food, where a few large companies control a significant portion of the market (Bivens & Mishel, 2013). In these industries, workers often face low wages, limited benefits, and reduced bargaining power, leading to a situation where they are dependent on the employer for their livelihood. This dependence can result in further suppression of wages and a decline in working conditions.\\n\\nOverall, the concept of monopsony is essential to understanding the dynamics of labor markets and the impact of market power on workers. Further research is needed to understand the extent and impact of monopsonies on the economy and to develop policies to address this issue.\\n\\nReferences:\\nBivens, J., & Mishel, L. (2013). The Pay of Corporate Executives and Financial Professionals as Evidence of Rents in Top 1 Percent Incomes. Journal of Economic Perspectives, 27(3), 57-78.',\n",
       " 'prompt': 'Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73c2bdf1-bb3e-40e1-a930-4c32d6c57636",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max source length: 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max target length: 512\n"
     ]
    }
   ],
   "source": [
    "# The maximum total input sequence length after tokenization. \n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\n",
    "tokenized_inputs = dataset.map(lambda x: tokenizer(x[\"prompt\"], truncation=True), batched=True)\n",
    "input_lenghts = [len(x) for x in tokenized_inputs[\"input_ids\"]]\n",
    "# take 85 percentile of max length for better utilization\n",
    "max_source_length = int(np.percentile(input_lenghts, 85))\n",
    "print(f\"Max source length: {max_source_length}\")\n",
    "\n",
    "# The maximum total sequence length for target text after tokenization. \n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\"\n",
    "tokenized_targets = dataset.map(lambda x: tokenizer(x[\"text\"], truncation=True), batched=True)\n",
    "target_lenghts = [len(x) for x in tokenized_targets[\"input_ids\"]]\n",
    "# take 90 percentile of max length for better utilization\n",
    "max_target_length = int(np.percentile(target_lenghts, 90))\n",
    "print(f\"Max target length: {max_target_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76632e53-9c72-49f2-82e7-bacde7d1ddb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(sample, padding=\"max_length\"):\n",
    "    inputs = sample[\"prompt\"]\n",
    "\n",
    "    # tokenize inputs\n",
    "    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    # Tokenize targets with the `text_target` keyword argument\n",
    "    labels = tokenizer(text_target=sample['text'], max_length=max_target_length, padding=padding, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f2f7d05-f52a-49b7-bed6-35b3b70d5338",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=['prompt', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a50a71ea-b979-48ff-96bc-a18b72e1e7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys of tokenized dataset: ['input_ids', 'attention_mask', 'labels']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Keys of tokenized dataset: {list(tokenized_dataset.features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c05662b-e2f7-40f5-8e98-b6cd4bb906b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [1072, 25, 1431, 3, 9, 710, 5302, 81, 8, 20208, 13, 8, 1657, 96, 2157, 9280, 106, 63, 121, 16, 1456, 7, 58, 863, 169, 4062, 1341, 12, 1055, 7414, 102, 739, 725, 16, 8, 12568, 512, 11, 3, 8464, 2193, 585, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [96, 9168, 9280, 106, 63, 121, 2401, 7, 12, 3, 9, 512, 1809, 213, 132, 19, 163, 80, 8001, 21, 3, 9, 1090, 207, 42, 313, 5, 86, 1456, 7, 6, 48, 1657, 19, 1989, 2193, 16, 8, 5347, 512, 6, 213, 3, 9, 7414, 102, 739, 63, 6152, 65, 1516, 579, 147, 8, 15488, 11, 464, 1124, 13, 70, 1652, 5, 37, 3053, 13, 3, 9, 7414, 102, 739, 63, 54, 741, 16, 1364, 15488, 11, 3915, 4311, 1645, 21, 2765, 6, 38, 8, 6152, 65, 385, 17821, 12, 993, 15488, 42, 370, 394, 464, 1124, 5, 17716, 585, 65, 4313, 1055, 7414, 102, 739, 725, 16, 5238, 224, 38, 3549, 11, 1006, 542, 6, 213, 3, 9, 360, 508, 688, 610, 3, 9, 1516, 4149, 13, 8, 512, 41, 279, 757, 29, 7, 3, 184, 8306, 88, 40, 6, 2038, 137, 86, 175, 5238, 6, 2765, 557, 522, 731, 15488, 6, 1643, 1393, 6, 11, 3915, 13435, 53, 579, 6, 1374, 12, 3, 9, 1419, 213, 79, 33, 8976, 30, 8, 6152, 21, 70, 27656, 5, 100, 24264, 54, 741, 16, 856, 23059, 13, 15488, 11, 3, 9, 7198, 16, 464, 1124, 5, 9126, 6, 8, 2077, 13, 7414, 102, 739, 63, 19, 1832, 12, 1705, 8, 14966, 13, 5347, 3212, 11, 8, 1113, 13, 512, 579, 30, 2765, 5, 9006, 585, 19, 906, 12, 734, 8, 5996, 11, 1113, 13, 7414, 102, 739, 725, 30, 8, 2717, 11, 12, 1344, 3101, 12, 1115, 48, 962, 5, 17881, 7, 10, 2106, 1926, 7, 6, 446, 5, 6, 3, 184, 8306, 88, 40, 6, 301, 5, 25195, 5, 37, 5077, 13, 11588, 5408, 7, 11, 5421, 4751, 7, 38, 24122, 13, 9405, 7, 16, 2224, 209, 1915, 3728, 20110, 7, 5, 3559, 13, 9071, 27631, 7, 6, 2307, 17867, 6, 3, 3436, 18, 3940, 5, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9c5d2f79-702c-4696-9351-7a297cea4585",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    }
   ],
   "source": [
    "# save datasets to disk for later easy loading\n",
    "tokenized_dataset.save_to_disk(\"data\", max_shard_size=\"90MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9bcaf823-7ba7-43be-8633-f22ee03fe2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = load_from_disk(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a000bf8-6780-4ddb-b167-b34991c2a4ed",
   "metadata": {},
   "source": [
    "## Create LoRA Config and new PEFT-Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "352f98b9-1119-49b1-9185-ef3d34fc4149",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    " r=16, \n",
    " lora_alpha=32,\n",
    " target_modules=[\"q\", \"v\"],\n",
    " lora_dropout=0.05,\n",
    " bias=\"none\",\n",
    " task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "703393b6-dee9-4674-947b-0e02c323909c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4718592 || all params: 787868672 || trainable%: 0.5989059049678777\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58516fb8-b752-40b9-aa89-312d91753ec2",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "91704827-0fc5-4253-a17e-5dd0066cbc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to ignore tokenizer pad token in the loss\n",
    "label_pad_token_id = -100\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1190f249-2a08-44ce-b81f-3cb613e3beee",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir=\"lora-flan-t5-large\"\n",
    "\n",
    "# Define training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    auto_find_batch_size=True,\n",
    "    learning_rate=1e-3, # higher learning rate\n",
    "    num_train_epochs=5,\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    save_strategy=\"no\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0cc69736-c107-4f66-baa9-7dca0f723f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Trainer instance\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ab23827e-3fd0-4e56-a5b4-8b3c685b801b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='132280' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [     2/132280 : < :, Epoch 0.00/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_45/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">2490965745.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 2&gt;</span>                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_45/2490965745.py'</span>                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1664</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">train</span>                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1661 │   │   </span>inner_training_loop = find_executable_batch_size(                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1662 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._inner_training_loop, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._train_batch_size, args.auto_find_batch_size  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1663 │   │   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1664 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> inner_training_loop(                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1665 │   │   │   </span>args=args,                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1666 │   │   │   </span>resume_from_checkpoint=resume_from_checkpoint,                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1667 │   │   │   </span>trial=trial,                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/accelerate/utils/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">memory.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">124</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">decorator</span>              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">121 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> batch_size == <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>:                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">122 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">RuntimeError</span>(<span style=\"color: #808000; text-decoration-color: #808000\">\"No executable batch size found, reached zero.\"</span>)        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">123 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>124 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> function(batch_size, *args, **kwargs)                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">125 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">Exception</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">as</span> e:                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">126 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> should_reduce_batch_size(e):                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">127 │   │   │   │   │   </span>gc.collect()                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1940</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_inner_training_loop</span>     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1937 │   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> model.no_sync():                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1938 │   │   │   │   │   │   </span>tr_loss_step = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.training_step(model, inputs)                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1939 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1940 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>tr_loss_step = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.training_step(model, inputs)                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1941 │   │   │   │   </span>                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1942 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> (                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1943 │   │   │   │   │   </span>args.logging_nan_inf_filter                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2753</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">training_step</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2750 │   │   │   # loss gets scaled under gradient_accumulation_steps in deepspeed</span>             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2751 │   │   │   </span>loss = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.deepspeed.backward(loss)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2752 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2753 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>loss.backward()                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2754 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2755 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> loss.detach()                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2756 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/torch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_tensor.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">487</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 484 │   │   │   │   </span>create_graph=create_graph,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 485 │   │   │   │   </span>inputs=inputs,                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 486 │   │   │   </span>)                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 487 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>torch.autograd.backward(                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 488 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, gradient, retain_graph, create_graph, inputs=inputs                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 489 │   │   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 490 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/torch/autograd/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">__init__.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">200</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">197 │   # The reason we repeat same the comment below is that</span>                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">198 │   # some Python versions print out the first line of a multi-line function</span>               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">199 │   # calls in the traceback and some print out the last line</span>                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>200 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>Variable._execution_engine.run_backward(  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to run the bac</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">201 │   │   </span>tensors, grad_tensors_, retain_graph, create_graph, inputs,                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">202 │   │   </span>allow_unreachable=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>, accumulate_grad=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to ru</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">203 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[31m╭─\u001B[0m\u001B[31m────────────────────────────── \u001B[0m\u001B[1;31mTraceback \u001B[0m\u001B[1;2;31m(most recent call last)\u001B[0m\u001B[31m ───────────────────────────────\u001B[0m\u001B[31m─╮\u001B[0m\n",
       "\u001B[31m│\u001B[0m \u001B[2;33m/tmp/ipykernel_45/\u001B[0m\u001B[1;33m2490965745.py\u001B[0m:\u001B[94m2\u001B[0m in \u001B[92m<cell line: 2>\u001B[0m                                              \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m \u001B[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_45/2490965745.py'\u001B[0m                           \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m \u001B[2;33m/opt/conda/lib/python3.10/site-packages/transformers/\u001B[0m\u001B[1;33mtrainer.py\u001B[0m:\u001B[94m1664\u001B[0m in \u001B[92mtrain\u001B[0m                    \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m   \u001B[2m1661 \u001B[0m\u001B[2m│   │   \u001B[0minner_training_loop = find_executable_batch_size(                                 \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m   \u001B[2m1662 \u001B[0m\u001B[2m│   │   │   \u001B[0m\u001B[96mself\u001B[0m._inner_training_loop, \u001B[96mself\u001B[0m._train_batch_size, args.auto_find_batch_size  \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m   \u001B[2m1663 \u001B[0m\u001B[2m│   │   \u001B[0m)                                                                                 \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m \u001B[31m❱ \u001B[0m1664 \u001B[2m│   │   \u001B[0m\u001B[94mreturn\u001B[0m inner_training_loop(                                                       \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m   \u001B[2m1665 \u001B[0m\u001B[2m│   │   │   \u001B[0margs=args,                                                                    \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m   \u001B[2m1666 \u001B[0m\u001B[2m│   │   │   \u001B[0mresume_from_checkpoint=resume_from_checkpoint,                                \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m   \u001B[2m1667 \u001B[0m\u001B[2m│   │   │   \u001B[0mtrial=trial,                                                                  \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m \u001B[2;33m/opt/conda/lib/python3.10/site-packages/accelerate/utils/\u001B[0m\u001B[1;33mmemory.py\u001B[0m:\u001B[94m124\u001B[0m in \u001B[92mdecorator\u001B[0m              \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m   \u001B[2m121 \u001B[0m\u001B[2m│   │   │   \u001B[0m\u001B[94mif\u001B[0m batch_size == \u001B[94m0\u001B[0m:                                                            \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m   \u001B[2m122 \u001B[0m\u001B[2m│   │   │   │   \u001B[0m\u001B[94mraise\u001B[0m \u001B[96mRuntimeError\u001B[0m(\u001B[33m\"\u001B[0m\u001B[33mNo executable batch size found, reached zero.\u001B[0m\u001B[33m\"\u001B[0m)        \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m   \u001B[2m123 \u001B[0m\u001B[2m│   │   │   \u001B[0m\u001B[94mtry\u001B[0m:                                                                           \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m \u001B[31m❱ \u001B[0m124 \u001B[2m│   │   │   │   \u001B[0m\u001B[94mreturn\u001B[0m function(batch_size, *args, **kwargs)                               \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m   \u001B[2m125 \u001B[0m\u001B[2m│   │   │   \u001B[0m\u001B[94mexcept\u001B[0m \u001B[96mException\u001B[0m \u001B[94mas\u001B[0m e:                                                         \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m   \u001B[2m126 \u001B[0m\u001B[2m│   │   │   │   \u001B[0m\u001B[94mif\u001B[0m should_reduce_batch_size(e):                                            \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m   \u001B[2m127 \u001B[0m\u001B[2m│   │   │   │   │   \u001B[0mgc.collect()                                                           \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m \u001B[2;33m/opt/conda/lib/python3.10/site-packages/transformers/\u001B[0m\u001B[1;33mtrainer.py\u001B[0m:\u001B[94m1940\u001B[0m in \u001B[92m_inner_training_loop\u001B[0m     \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m   \u001B[2m1937 \u001B[0m\u001B[2m│   │   │   │   │   \u001B[0m\u001B[94mwith\u001B[0m model.no_sync():                                                 \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m   \u001B[2m1938 \u001B[0m\u001B[2m│   │   │   │   │   │   \u001B[0mtr_loss_step = \u001B[96mself\u001B[0m.training_step(model, inputs)                  \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m   \u001B[2m1939 \u001B[0m\u001B[2m│   │   │   │   \u001B[0m\u001B[94melse\u001B[0m:                                                                     \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m \u001B[31m❱ \u001B[0m1940 \u001B[2m│   │   │   │   │   \u001B[0mtr_loss_step = \u001B[96mself\u001B[0m.training_step(model, inputs)                      \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m   \u001B[2m1941 \u001B[0m\u001B[2m│   │   │   │   \u001B[0m                                                                          \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m   \u001B[2m1942 \u001B[0m\u001B[2m│   │   │   │   \u001B[0m\u001B[94mif\u001B[0m (                                                                      \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m   \u001B[2m1943 \u001B[0m\u001B[2m│   │   │   │   │   \u001B[0margs.logging_nan_inf_filter                                           \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m \u001B[2;33m/opt/conda/lib/python3.10/site-packages/transformers/\u001B[0m\u001B[1;33mtrainer.py\u001B[0m:\u001B[94m2753\u001B[0m in \u001B[92mtraining_step\u001B[0m            \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m   \u001B[2m2750 \u001B[0m\u001B[2m│   │   │   \u001B[0m\u001B[2m# loss gets scaled under gradient_accumulation_steps in deepspeed\u001B[0m             \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m   \u001B[2m2751 \u001B[0m\u001B[2m│   │   │   \u001B[0mloss = \u001B[96mself\u001B[0m.deepspeed.backward(loss)                                          \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m   \u001B[2m2752 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[94melse\u001B[0m:                                                                             \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m \u001B[31m❱ \u001B[0m2753 \u001B[2m│   │   │   \u001B[0mloss.backward()                                                               \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m   \u001B[2m2754 \u001B[0m\u001B[2m│   │   \u001B[0m                                                                                  \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m   \u001B[2m2755 \u001B[0m\u001B[2m│   │   \u001B[0m\u001B[94mreturn\u001B[0m loss.detach()                                                              \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m   \u001B[2m2756 \u001B[0m                                                                                          \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m \u001B[2;33m/opt/conda/lib/python3.10/site-packages/torch/\u001B[0m\u001B[1;33m_tensor.py\u001B[0m:\u001B[94m487\u001B[0m in \u001B[92mbackward\u001B[0m                         \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m   \u001B[2m 484 \u001B[0m\u001B[2m│   │   │   │   \u001B[0mcreate_graph=create_graph,                                                \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m   \u001B[2m 485 \u001B[0m\u001B[2m│   │   │   │   \u001B[0minputs=inputs,                                                            \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m   \u001B[2m 486 \u001B[0m\u001B[2m│   │   │   \u001B[0m)                                                                             \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m \u001B[31m❱ \u001B[0m 487 \u001B[2m│   │   \u001B[0mtorch.autograd.backward(                                                          \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m   \u001B[2m 488 \u001B[0m\u001B[2m│   │   │   \u001B[0m\u001B[96mself\u001B[0m, gradient, retain_graph, create_graph, inputs=inputs                     \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m   \u001B[2m 489 \u001B[0m\u001B[2m│   │   \u001B[0m)                                                                                 \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m   \u001B[2m 490 \u001B[0m                                                                                          \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m \u001B[2;33m/opt/conda/lib/python3.10/site-packages/torch/autograd/\u001B[0m\u001B[1;33m__init__.py\u001B[0m:\u001B[94m200\u001B[0m in \u001B[92mbackward\u001B[0m               \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m   \u001B[2m197 \u001B[0m\u001B[2m│   \u001B[0m\u001B[2m# The reason we repeat same the comment below is that\u001B[0m                                  \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m   \u001B[2m198 \u001B[0m\u001B[2m│   \u001B[0m\u001B[2m# some Python versions print out the first line of a multi-line function\u001B[0m               \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m   \u001B[2m199 \u001B[0m\u001B[2m│   \u001B[0m\u001B[2m# calls in the traceback and some print out the last line\u001B[0m                              \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m \u001B[31m❱ \u001B[0m200 \u001B[2m│   \u001B[0mVariable._execution_engine.run_backward(  \u001B[2m# Calls into the C++ engine to run the bac\u001B[0m   \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m   \u001B[2m201 \u001B[0m\u001B[2m│   │   \u001B[0mtensors, grad_tensors_, retain_graph, create_graph, inputs,                        \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m   \u001B[2m202 \u001B[0m\u001B[2m│   │   \u001B[0mallow_unreachable=\u001B[94mTrue\u001B[0m, accumulate_grad=\u001B[94mTrue\u001B[0m)  \u001B[2m# Calls into the C++ engine to ru\u001B[0m   \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m   \u001B[2m203 \u001B[0m                                                                                           \u001B[31m│\u001B[0m\n",
       "\u001B[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001B[0m\n",
       "\u001B[1;91mKeyboardInterrupt\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train model\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}